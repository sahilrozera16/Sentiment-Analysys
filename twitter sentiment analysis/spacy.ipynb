{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CSV file\n",
    "train = pd.read_csv ('train_E6oV3lV.csv')\n",
    "test = pd.read_csv ('test_tweets_anuFYb8.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Both Datasets\n",
    "combine = train.append(test,ignore_index=True,sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...\n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3    0.0                                bihday your majesty\n",
       "3   4    0.0  #model   i love u take with u all the time in ...\n",
       "4   5    0.0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 5 records\n",
    "combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>49155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thought factory: left-right polarisation! #tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>49156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feeling like a mermaid ð #hairflip #neverre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>49157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#hillary #campaigned today in #ohio((omg)) &amp;am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>49158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happy, at work conference: right mindset leads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>49159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my   song \"so glad\" free download!  #shoegaze ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet\n",
       "49154  49155    NaN  thought factory: left-right polarisation! #tru...\n",
       "49155  49156    NaN  feeling like a mermaid ð #hairflip #neverre...\n",
       "49156  49157    NaN  #hillary #campaigned today in #ohio((omg)) &am...\n",
       "49157  49158    NaN  happy, at work conference: right mindset leads...\n",
       "49158  49159    NaN  my   song \"so glad\" free download!  #shoegaze ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last 5 records\n",
    "combine.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of dataframe\n",
    "combine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49159 entries, 0 to 49158\n",
      "Data columns (total 3 columns):\n",
      "id       49159 non-null int64\n",
      "label    31962 non-null float64\n",
      "tweet    49159 non-null object\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# View data information\n",
    "combine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feedback Value count of training dataset\n",
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokening the Data With spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we’re working with, let’s create a custom tokenizer function using `spaCy`. We’ll use this function to automatically strip information we don’t need, like stopwords and punctuation, from each review.\n",
    "\n",
    "We’ll start by importing the English models we need from `spaCy`, as well as Python’s `string` module, which contains a helpful list of all punctuation marks that we can use in `string.punctuation`. We’ll create variables that contain the punctuation marks and stopwords we want to remove, and a parser that runs input through `spaCy‘s` English module.\n",
    "\n",
    "Then, we’ll create a `spacy_tokenizer()` function that accepts a sentence as input and processes the sentence into tokens, performing lemmatization, lowercasing, and removing stop words. This is similar to what we did in the examples earlier in this tutorial, but now we’re putting it all together into a single function for preprocessing each user review we’re analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Custom Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further clean our text data, we’ll also want to create a custom transformer for removing initial and end spaces and converting text into lower case. Here, we will create a custom `predictors` class wich inherits the `TransformerMixin` class. This class overrides the transform, fit and get_parrams methods. We’ll also create a `clean_text()` function that removes spaces and converts text into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we classify text, we end up with text snippets matched with their respective labels. But we can’t simply use text strings in our machine learning model; we need a way to convert our text into something that can be represented numerically just like the labels (0 for positive and 1 for negative) are. Classifying text in positive and negative labels is called sentiment analysis. So we need a way to represent our text numerically.\n",
    "\n",
    "One tool we can use for doing this is called `Bag of Words`. BoW converts text into the matrix of occurrence of words within a given document. It focuses on whether given words occurred or not in the document, and it generates a matrix that we might see referred to as a BoW matrix or a document term matrix.\n",
    "\n",
    "We can generate a BoW matrix for our text data by using scikit-learn‘s CountVectorizer. In the code below, we’re telling `CountVectorizer` to use the custom `spacy_tokenizer` function we built as its tokenizer, and defining the ngram range we want.\n",
    "\n",
    "`N-grams` are combinations of adjacent words in a given text, where `n` is the number of words that incuded in the tokens. for example, in the sentence “Who will win the football world cup in 2022?” unigrams would be a sequence of single words such as “who”, “will”, “win” and so on. Bigrams would be a sequence of 2 contiguous words such as “who will”, “will win”, and so on. So the `ngram_range` parameter we’ll use in the code below sets the lower and upper bounds of the our `ngrams` (we’ll be using unigrams). Then we’ll assign the ngrams to bow_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting The Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train['tweet'] # the features we want to analyze\n",
    "y = train['label'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Pipeline and Generating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’re all set up, it’s time to actually build our model! We’ll start by importing the LogisticRegression module and creating a `LogisticRegression` classifier object.\n",
    "\n",
    "Then, we’ll create a `pipeline` with three components: a cleaner, a vectorizer, and a classifier. The `cleaner` uses our predictors class object to clean and preprocess the text. The `vectorizer` uses countvector objects to create the bag of words matrix for our text. The `classifier` is an object that performs the logistic regression to classify the sentiments.\n",
    "\n",
    "Once this pipeline is built, we’ll fit the pipeline components using fit()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', <__main__.predictors object at 0x000002B38FC57C48>), ('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "    ...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "classifier = LinearSVC()\n",
    "\n",
    "\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,classification_report,confusion_matrix\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      8920\n",
      "           1       0.87      0.62      0.73       669\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      9589\n",
      "   macro avg       0.92      0.81      0.85      9589\n",
      "weighted avg       0.97      0.97      0.96      9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7255244755244755"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        #studiolife #aislife #requires #passion #dedic...\n",
       "1         @user #white #supremacists want everyone to s...\n",
       "2        safe ways to heal your #acne!!    #altwaystohe...\n",
       "3        is the hp and the cursed child book up for res...\n",
       "4          3rd #bihday to my amazing, hilarious #nephew...\n",
       "5                              choose to be   :) #momtips \n",
       "6        something inside me dies ð¦ð¿â¨  eyes nes...\n",
       "7        #finished#tattoo#inked#ink#loveitâ¤ï¸ #â¤ï¸...\n",
       "8         @user @user @user i will never understand why...\n",
       "9        #delicious   #food #lovelife #capetown mannaep...\n",
       "10       1000dayswasted - narcosis infinite ep.. make m...\n",
       "11       one of the world's greatest spoing events   #l...\n",
       "12       half way through the website now and #allgoing...\n",
       "13       good food, good life , #enjoy and   ððð...\n",
       "14       i'll stand behind this #guncontrolplease   #se...\n",
       "15       i ate,i ate and i ate...ðð   #jamaisasth...\n",
       "16        @user got my @user limited edition rain or sh...\n",
       "17       &amp; #love &amp; #hugs &amp; #kisses too! how...\n",
       "18       ð­ðð #girls   #sun #fave @ london, uni...\n",
       "19       thought factory: bbc neutrality on right wing ...\n",
       "20       hey guys tommorow is the last day of my exams ...\n",
       "21        @user @user  @user  #levyrroni #recuerdos mem...\n",
       "22       my mind is like ððð½ð but my body l...\n",
       "23       never been this down on myself in my entire li...\n",
       "24       check  twitterww - trends: \"trending worldwide...\n",
       "25       i thought i saw a mermaid!!! #ceegee  #smcr   ...\n",
       "26                   chick gets fucked hottest naked lady \n",
       "27       happy bday lucyâ¨â¨ð xoxo #love #beautifu...\n",
       "28        haroldfriday have a weekend filled with sunbe...\n",
       "29       @user @user tried that! but nothing - will try...\n",
       "                               ...                        \n",
       "17167    people do anything for fucking attention nowad...\n",
       "17168    creative bubble got burst ð¢ looking forward...\n",
       "17169    tomorrow is gonna be a big day! we are going t...\n",
       "17170    i am thankful for baby giggles. #thankful #pos...\n",
       "17171    #model   i love u take with u all the time in ...\n",
       "17172    in life u will grow to learn some pple will wo...\n",
       "17173    ði was the storm,you were the rain. togethe...\n",
       "17174    lovelgq -  broken ep via   #rnb #love #heabrok...\n",
       "17175    spread love not hateâ¤ï¸ðððð #pr...\n",
       "17176       @user @user are the most racist pay ever!!!!! \n",
       "17177    i am thankful for children. #thankful #positiv...\n",
       "17178    liverpool â¤ï¸ð¬ð§ #walk #liverpool #sta...\n",
       "17179    #bakersfield   rooster simulation: i want to c...\n",
       "17180    por do sol ó¾â¤ï¸#instagood #beautiful   #...\n",
       "17181    @user hell yeah what a great surprise for your...\n",
       "17182    when ur the joke ur defensive towards everythi...\n",
       "17183    #enjoying the #evening #sun in my #bedroom â¨...\n",
       "17184    tonight on @user from 9pm gmt  you can here a ...\n",
       "17185    today is a good day for excercise #imready #so...\n",
       "17186    good night with a tea and music âï¸ðð...\n",
       "17187    loving lifeðºð¸âï¸ð  #createyourfutu...\n",
       "17188    black professor demonizes, proposes nazi style...\n",
       "17189    learn how to think positive.  #positive   #ins...\n",
       "17190    we love the pretty, happy and fresh you! #teen...\n",
       "17191    2_damn_tuff-ruff_muff__techno_city-(ng005)-web...\n",
       "17192    thought factory: left-right polarisation! #tru...\n",
       "17193    feeling like a mermaid ð #hairflip #neverre...\n",
       "17194    #hillary #campaigned today in #ohio((omg)) &am...\n",
       "17195    happy, at work conference: right mindset leads...\n",
       "17196    my   song \"so glad\" free download!  #shoegaze ...\n",
       "Name: tweet, Length: 17197, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweet = test['tweet']\n",
    "test_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pipe.predict(test_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'] = test_pred\n",
    "\n",
    "submission = test[['id','label']]\n",
    "\n",
    "submission.to_csv('result_spacy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
